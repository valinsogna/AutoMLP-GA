{'nb_neurons': 256, 'nb_layers': 5, 'activation': 'ReLU', 'optimizer': 'adam', 'lr_scheduler': 'cosine', 'initial_lr': 0.0001}
{'nb_neurons': 256, 'nb_layers': 1, 'activation': 'LeakyReLU', 'optimizer': 'adamw', 'lr_scheduler': 'exponential', 'initial_lr': 0.01}
